{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "055a91df-f333-4e2e-bb13-27f871b1e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app_if.py\n"
     ]
    }
   ],
   "source": [
    "# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 app.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Kafka config\n",
    "SERVER = \"broker:9092\"\n",
    "TOPIC = \"pm10\"\n",
    "CHECKPOINT_PATH = \"/home/jovyan/notebooks/spark/checkpoints\"\n",
    "ANOMALY_PARQUET_PATH = \"/home/jovyan/notebooks/spark/anomalies/iforest\"\n",
    "\n",
    "# Schemat danych z Kafka (JSON jako string)\n",
    "SCHEMA = StructType([\n",
    "    StructField(\"station_id\", IntegerType()),\n",
    "    StructField(\"reading_date\", StringType()),\n",
    "    StructField(\"datetime_from_sensor\", TimestampType()),\n",
    "    StructField(\"value\", DoubleType()),\n",
    "    StructField(\"unit\", StringType())\n",
    "])\n",
    "\n",
    "# Wczytaj wytrenowany model Isolation Forest\n",
    "isoforest_model = joblib.load(\"/home/jovyan/notebooks/spark/isoforest_model.pkl\")\n",
    "\n",
    "# UDF do predykcji\n",
    "def predict_anomaly(value):\n",
    "    if value is None:\n",
    "        return 0\n",
    "    try:\n",
    "        prediction = isoforest_model.predict([[value]])\n",
    "        return int(prediction[0] == -1)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "predict_anomaly_udf = udf(predict_anomaly, IntegerType())\n",
    "\n",
    "# Funkcja wykrywania anomalii przez Isolation Forest\n",
    "def detect_isoforest_anomalies(df, epoch_id):\n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"‚ö†Ô∏è Batch {epoch_id} is empty ‚Äì skipping.\")\n",
    "        return\n",
    "    try:\n",
    "        print(f\"üå≤ [Batch {epoch_id}] Isolation Forest anomaly detection...\")\n",
    "        df_with_anomalies = df.withColumn(\"anomaly\", predict_anomaly_udf(col(\"value\")))\n",
    "        anomalies = df_with_anomalies.filter(col(\"anomaly\") == 1)\n",
    "\n",
    "        if anomalies.rdd.isEmpty():\n",
    "            print(f\"‚ÑπÔ∏è No anomalies in batch {epoch_id}\")\n",
    "            return\n",
    "\n",
    "        anomalies.select(\"station_id\", \"datetime_from_sensor\", \"value\").show(truncate=False)\n",
    "\n",
    "        # Zapisz do pliku Parquet\n",
    "        anomalies.write.mode(\"append\").parquet(ANOMALY_PARQUET_PATH)\n",
    "\n",
    "        # Wy≈õlij do Kafka topic\n",
    "        kafka_ready = anomalies.selectExpr(\n",
    "            \"CAST(station_id AS STRING) AS key\",\n",
    "            \"to_json(struct(*)) AS value\"\n",
    "        )\n",
    "        kafka_ready.write.format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", SERVER) \\\n",
    "            .option(\"topic\", \"if_anomalies\") \\\n",
    "            .save()\n",
    "\n",
    "        print(\"üì§ Anomalie wys≈Çane do Kafka topic: 'if_anomalies'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during ISOForest detection: {e}\")\n",
    "\n",
    "# G≈Ç√≥wna aplikacja\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"IsolationForest-PM10\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # Dane z Kafka\n",
    "    raw = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", SERVER)\n",
    "        .option(\"subscribe\", TOPIC)\n",
    "        .option(\"startingOffsets\", \"latest\")\n",
    "        .option(\"failOnDataLoss\", \"false\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    parsed = (\n",
    "        raw.selectExpr(\"CAST(value AS STRING) AS json_str\")\n",
    "        .select(from_json(\"json_str\", SCHEMA).alias(\"data\"))\n",
    "        .select(\"data.*\")\n",
    "    )\n",
    "\n",
    "    # Stream wykrywajƒÖcy anomalie Isolation Forest\n",
    "    isoforest_query = (\n",
    "        parsed.writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .foreachBatch(detect_isoforest_anomalies)\n",
    "        .option(\"checkpointLocation\", CHECKPOINT_PATH + \"/iforest_detection\")\n",
    "        .trigger(processingTime=\"5 minutes\")\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446bea4-ce13-4c84-946c-1257829dad42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
